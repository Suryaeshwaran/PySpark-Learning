#### Day19: Resource Management & Job Scheduling
---

#### 1. Understand Spark Resources
- Spark jobs need CPU and Memory.
- These are split between:
	- Driver â†’ coordinates the job.
	- Executors â†’ do the actual work.

#### 2. Key Resource Configurations

When you run spark-submit, you can control resources:
``` bash
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 3 \
  --executor-cores 2 \
  --executor-memory 2G \
  --driver-memory 1G \
  wordcount.py
```
Details:
``` bash
--num-executors â†’ total executors requested.
--executor-cores â†’ CPU cores per executor.
--executor-memory â†’ memory per executor (excluding overhead).
--driver-memory â†’ memory for the driver.
```

ðŸ‘‰ Example above requests:

- 3 executors Ã— 2 cores = 6 cores total.
- 3 executors Ã— 2G memory = 6G memory total.

#### 3. Dynamic Allocation

Instead of fixing executors, Spark can scale up/down automatically:

Enable in spark-defaults.conf or with --conf:
``` bash
--conf spark.dynamicAllocation.enabled=true \
--conf spark.shuffle.service.enabled=true \
--conf spark.dynamicAllocation.minExecutors=2 \
--conf spark.dynamicAllocation.maxExecutors=10 \
--conf spark.dynamicAllocation.initialExecutors=2

```

ðŸ”‘ Requires shuffle service running in YARN.

#### 4. Scheduling Modes

When multiple Spark jobs run:

- FIFO (First In First Out) â†’ default, jobs run in order of submission.
- Fair Scheduler â†’ resources shared fairly across jobs.

In YARN:

- You can set queues (production, dev, etc.).
- Each queue can have resource limits.

#### 5. Monitoring

- Spark UI (http://localhost:4040) â†’ check executors, memory, tasks.
- YARN ResourceManager UI (http://localhost:8088) â†’ check resource allocation across all jobs.

**Exercise:**

- Run thru deploy-mode client & cluster
``` bash
PYSPARK_PYTHON=$(which python3) \
PYSPARK_DRIVER_PYTHON=$(which python3) \
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 1 \
  --executor-cores 1 \
  --executor-memory 1G \
  --driver-memory 1G \
  wordcount.py

PYSPARK_PYTHON=$(which python3) \
PYSPARK_DRIVER_PYTHON=$(which python3) \
spark-submit \
  --master yarn \
  --deploy-mode client \
  --num-executors 1 \
  --executor-cores 1 \
  --executor-memory 1G \
  --driver-memory 1G \
spark.py
```

- Run with Dynamic Allocation

``` bash
PYSPARK_PYTHON=$(which python3) \
PYSPARK_DRIVER_PYTHON=$(which python3) \
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.shuffle.service.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=1 \
  --conf spark.dynamicAllocation.maxExecutors=5 \
wordcount.py
```