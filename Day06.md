#### Day06:  Introduction to PySpark
---

#### What is PySpark?
- Apache Spark is a **powerful open-source data processing engine** written in Scala, designed for large-scale data processing.
- To support Python with Spark, **Apache Spark community released** a tool, **PySpark.** 
- Using PySpark, **you can work with RDDs**(Resilient Distributed Datasets) in Python programming language also.
- PySpark is the **Python API for Apache Spark.** It allows you to interact/interface with **Spark's distributed computation framework** using Python, **making it easier to work with big data** in a language many data scientists and engineers are familiar with. 
- By using PySpark, you can **create and manage Spark jobs,** and perform **complex data transformations and analysis.**

#### Why PySpark?
- The primary **purpose** of PySpark is to **enable processing of large-scale datasets in real-time** across a distributed computing environment **using Python.** 
- PySpark **provides an interface for interacting with Spark's core functionalities,** such as working with Resilient Distributed Datasets (RDDs) and DataFrames, using the Python programming language.
